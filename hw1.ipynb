{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNmPG+/cUFx4liKNa6ql3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wayu0730/advanced-algorithm/blob/main/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic**"
      ],
      "metadata": {
        "id": "IyUs9wQzTtum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZYUOroxROfa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in, device = device)\n",
        "y = torch.randn(N, D_out, device = device)\n",
        "w1 = torch.randn(D_in, H, device = device)\n",
        "w2 = torch.randn(H, D_out, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  h = x.mm(w1)\n",
        "  h_relu = h.clamp(min=0)\n",
        "  y_pred = h_relu.mm(w2)\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "metadata": {
        "id": "ok2NVmpdRsfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advance**"
      ],
      "metadata": {
        "id": "M9ObJAfWTzuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cpu')\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "w1 = torch.randn(D_in, H, requires_grad = True)\n",
        "w2 = torch.randn(H, D_out, requires_grad = True)"
      ],
      "metadata": {
        "id": "KYCNj1y7S8c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  h = x.mm(w1)\n",
        "  h_relu = h.clamp(min=0)\n",
        "  y_pred = h_relu.mm(w2)\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "metadata": {
        "id": "ogDuFjDlTBjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NN wrapper"
      ],
      "metadata": {
        "id": "Z0nGXTKo4vpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "model = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(H, D_out))\n",
        "\n",
        "learning_rate = 1e-6\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "for t in range(500):\n",
        "  \n",
        "  y_pred = model(x)\n",
        "  loss = torch.nn.functional.mse_loss(y_pred, y)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "70bjaVyD4ypz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv = nn.Conv2d(1, 32, 3)\n",
        "        self.dropout = nn.Dropout2d(0.25)\n",
        "        self.fc = nn.Linear(5408, 10) # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        output = F.log_softmax(x, dim=1) # log prob for numerical stability\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, epochs, log_interval):\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Clear gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward propagation\n",
        "            output = model(data)\n",
        "\n",
        "            # Negative log likelihood loss (log prob + nll loss = prob + cross entropy loss)\n",
        "            loss = F.nll_loss(output, target)\n",
        "\n",
        "            # Back propagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Parameter update\n",
        "            optimizer.step()\n",
        "\n",
        "            # Log training info\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
        "        for data, target in test_loader:\n",
        "            # Prediction\n",
        "            output = model(data)\n",
        "\n",
        "            # Compute loss & accuracy\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Log testing info\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 2\n",
        "    LOG_INTERVAL = 10\n",
        "\n",
        "    # Define image transform\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,)) # mean and std for the MNIST training set\n",
        "    ])\n",
        "\n",
        "    # Load dataset\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    test_dataset = datasets.MNIST('./data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Create network & optimizer\n",
        "    model = Net()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Train\n",
        "    train(model, train_loader, optimizer, EPOCHS, LOG_INTERVAL)\n",
        "\n",
        "    # Save and load model (for reference in case you are separating train and test files)\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "    model = Net()\n",
        "    model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
        "\n",
        "    # Test\n",
        "    test(model, test_loader)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "U0uzQbXD54ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hHt6qJO_DsE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "class TwoLyerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoLyerNet, self).__init__()\n",
        "        self.linear1 = nn.Linear()\n",
        "        self.fc = nn.Linear(5408, 10) # 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        output = F.log_softmax(x, dim=1) # log prob for numerical stability\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, epochs, log_interval):\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Clear gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward propagation\n",
        "            output = model(data)\n",
        "\n",
        "            # Negative log likelihood loss (log prob + nll loss = prob + cross entropy loss)\n",
        "            loss = F.nll_loss(output, target)\n",
        "\n",
        "            # Back propagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Parameter update\n",
        "            optimizer.step()\n",
        "\n",
        "            # Log training info\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad(): # disable gradient calculation for efficiency\n",
        "        for data, target in test_loader:\n",
        "            # Prediction\n",
        "            output = model(data)\n",
        "\n",
        "            # Compute loss & accuracy\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # how many predictions in this batch are correct\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Log testing info\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 2\n",
        "    LOG_INTERVAL = 10\n",
        "\n",
        "    # Define image transform\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,)) # mean and std for the MNIST training set\n",
        "    ])\n",
        "\n",
        "    # Load dataset\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    test_dataset = datasets.MNIST('./data', train=False,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Create network & optimizer\n",
        "    model = Net()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Train\n",
        "    train(model, train_loader, optimizer, EPOCHS, LOG_INTERVAL)\n",
        "\n",
        "    # Save and load model (for reference in case you are separating train and test files)\n",
        "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "    model = Net()\n",
        "    model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
        "\n",
        "    # Test\n",
        "    test(model, test_loader)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZX-fud8WFS8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# 定義神經網路模型\n",
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# 設定超參數\n",
        "input_size = 28 * 28  # MNIST圖像的維度\n",
        "hidden_size = 128  # 隱藏層的神經元數量\n",
        "num_classes = 10  # MNIST有10個類別\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# 載入MNIST數據集\n",
        "train_dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 初始化神經網路模型\n",
        "model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "# 定義損失函數和優化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 訓練神經網路模型\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # 將圖像攤平成向量\n",
        "        images = images.reshape(-1, input_size)\n",
        "\n",
        "        # 前向傳播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 反向傳播和優化\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
        "\n",
        "# 在測試集上評估模型\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, input_size)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isvD4X09FE8N",
        "outputId": "418f4f06-10fb-4769-82d8-aa34ac618747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/938], Loss: 0.4718\n",
            "Epoch [1/10], Step [200/938], Loss: 0.3996\n",
            "Epoch [1/10], Step [300/938], Loss: 0.3629\n",
            "Epoch [1/10], Step [400/938], Loss: 0.1389\n",
            "Epoch [1/10], Step [500/938], Loss: 0.3152\n",
            "Epoch [1/10], Step [600/938], Loss: 0.1657\n",
            "Epoch [1/10], Step [700/938], Loss: 0.3751\n",
            "Epoch [1/10], Step [800/938], Loss: 0.1908\n",
            "Epoch [1/10], Step [900/938], Loss: 0.0661\n",
            "Epoch [2/10], Step [100/938], Loss: 0.4095\n",
            "Epoch [2/10], Step [200/938], Loss: 0.2047\n",
            "Epoch [2/10], Step [300/938], Loss: 0.1396\n",
            "Epoch [2/10], Step [400/938], Loss: 0.0507\n",
            "Epoch [2/10], Step [500/938], Loss: 0.2214\n",
            "Epoch [2/10], Step [600/938], Loss: 0.1172\n",
            "Epoch [2/10], Step [700/938], Loss: 0.1014\n",
            "Epoch [2/10], Step [800/938], Loss: 0.2870\n",
            "Epoch [2/10], Step [900/938], Loss: 0.0969\n",
            "Epoch [3/10], Step [100/938], Loss: 0.1921\n",
            "Epoch [3/10], Step [200/938], Loss: 0.0805\n",
            "Epoch [3/10], Step [300/938], Loss: 0.0277\n",
            "Epoch [3/10], Step [400/938], Loss: 0.0403\n",
            "Epoch [3/10], Step [500/938], Loss: 0.0491\n",
            "Epoch [3/10], Step [600/938], Loss: 0.1027\n",
            "Epoch [3/10], Step [700/938], Loss: 0.1872\n",
            "Epoch [3/10], Step [800/938], Loss: 0.2827\n",
            "Epoch [3/10], Step [900/938], Loss: 0.1096\n",
            "Epoch [4/10], Step [100/938], Loss: 0.0431\n",
            "Epoch [4/10], Step [200/938], Loss: 0.1282\n",
            "Epoch [4/10], Step [300/938], Loss: 0.0395\n",
            "Epoch [4/10], Step [400/938], Loss: 0.1215\n",
            "Epoch [4/10], Step [500/938], Loss: 0.1261\n",
            "Epoch [4/10], Step [600/938], Loss: 0.2070\n",
            "Epoch [4/10], Step [700/938], Loss: 0.0396\n",
            "Epoch [4/10], Step [800/938], Loss: 0.0952\n",
            "Epoch [4/10], Step [900/938], Loss: 0.0632\n",
            "Epoch [5/10], Step [100/938], Loss: 0.0667\n",
            "Epoch [5/10], Step [200/938], Loss: 0.1404\n",
            "Epoch [5/10], Step [300/938], Loss: 0.0709\n",
            "Epoch [5/10], Step [400/938], Loss: 0.0366\n",
            "Epoch [5/10], Step [500/938], Loss: 0.0728\n",
            "Epoch [5/10], Step [600/938], Loss: 0.0169\n",
            "Epoch [5/10], Step [700/938], Loss: 0.0671\n",
            "Epoch [5/10], Step [800/938], Loss: 0.0230\n",
            "Epoch [5/10], Step [900/938], Loss: 0.0335\n",
            "Epoch [6/10], Step [100/938], Loss: 0.0293\n",
            "Epoch [6/10], Step [200/938], Loss: 0.0643\n",
            "Epoch [6/10], Step [300/938], Loss: 0.0298\n",
            "Epoch [6/10], Step [400/938], Loss: 0.1187\n",
            "Epoch [6/10], Step [500/938], Loss: 0.0189\n",
            "Epoch [6/10], Step [600/938], Loss: 0.0564\n",
            "Epoch [6/10], Step [700/938], Loss: 0.0342\n",
            "Epoch [6/10], Step [800/938], Loss: 0.0894\n",
            "Epoch [6/10], Step [900/938], Loss: 0.0112\n",
            "Epoch [7/10], Step [100/938], Loss: 0.0364\n",
            "Epoch [7/10], Step [200/938], Loss: 0.0152\n",
            "Epoch [7/10], Step [300/938], Loss: 0.0548\n",
            "Epoch [7/10], Step [400/938], Loss: 0.0459\n",
            "Epoch [7/10], Step [500/938], Loss: 0.0326\n",
            "Epoch [7/10], Step [600/938], Loss: 0.0381\n",
            "Epoch [7/10], Step [700/938], Loss: 0.1706\n",
            "Epoch [7/10], Step [800/938], Loss: 0.0718\n",
            "Epoch [7/10], Step [900/938], Loss: 0.0522\n",
            "Epoch [8/10], Step [100/938], Loss: 0.0673\n",
            "Epoch [8/10], Step [200/938], Loss: 0.0183\n",
            "Epoch [8/10], Step [300/938], Loss: 0.0112\n",
            "Epoch [8/10], Step [400/938], Loss: 0.1210\n",
            "Epoch [8/10], Step [500/938], Loss: 0.0167\n",
            "Epoch [8/10], Step [600/938], Loss: 0.0047\n",
            "Epoch [8/10], Step [700/938], Loss: 0.0574\n",
            "Epoch [8/10], Step [800/938], Loss: 0.0326\n",
            "Epoch [8/10], Step [900/938], Loss: 0.0230\n",
            "Epoch [9/10], Step [100/938], Loss: 0.0074\n",
            "Epoch [9/10], Step [200/938], Loss: 0.0069\n",
            "Epoch [9/10], Step [300/938], Loss: 0.0213\n",
            "Epoch [9/10], Step [400/938], Loss: 0.0162\n",
            "Epoch [9/10], Step [500/938], Loss: 0.0129\n",
            "Epoch [9/10], Step [600/938], Loss: 0.0219\n",
            "Epoch [9/10], Step [700/938], Loss: 0.0143\n",
            "Epoch [9/10], Step [800/938], Loss: 0.0663\n",
            "Epoch [9/10], Step [900/938], Loss: 0.0123\n",
            "Epoch [10/10], Step [100/938], Loss: 0.0136\n",
            "Epoch [10/10], Step [200/938], Loss: 0.0690\n",
            "Epoch [10/10], Step [300/938], Loss: 0.0293\n",
            "Epoch [10/10], Step [400/938], Loss: 0.0326\n",
            "Epoch [10/10], Step [500/938], Loss: 0.0079\n",
            "Epoch [10/10], Step [600/938], Loss: 0.0035\n",
            "Epoch [10/10], Step [700/938], Loss: 0.0491\n",
            "Epoch [10/10], Step [800/938], Loss: 0.0019\n",
            "Epoch [10/10], Step [900/938], Loss: 0.0282\n",
            "Test Accuracy of the model on the 10000 test images: 97.75 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# 載入MNIST手寫數字資料集，並將資料轉換為Tensor型態\n",
        "train_data = torchvision.datasets.MNIST(root='./data', train=True, \n",
        "                                        transform=transforms.ToTensor(), \n",
        "                                        download=True)\n",
        "test_data = torchvision.datasets.MNIST(root='./data', train=False, \n",
        "                                       transform=transforms.ToTensor(), \n",
        "                                       download=True)\n",
        "\n",
        "# 定義資料集分割比例\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "\n",
        "# 計算分割點\n",
        "train_size = int(train_ratio * len(train_data))\n",
        "test_size = len(train_data) - train_size\n",
        "\n",
        "# 將訓練集和驗證集合併為訓練集\n",
        "train_data, val_data = torch.utils.data.random_split(train_data, [train_size, test_size])\n",
        "\n",
        "# 定義DataLoader，用於將資料集轉換為小批次數據\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "pwdIY0y6NAuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 轉換訓練資料\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# 載入MNIST手寫數字資料集，並將資料轉換為Tensor型態\n",
        "train_data = torchvision.datasets.MNIST(root='./data', train=True, \n",
        "                                        transform=transforms.ToTensor(), \n",
        "                                        download=True)\n",
        "test_data = torchvision.datasets.MNIST(root='./data', train=False, \n",
        "                                       transform=transforms.ToTensor(), \n",
        "                                       download=True)\n",
        "\n",
        "# 定義資料集分割比例\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "\n",
        "# 計算分割點\n",
        "train_size = int(train_ratio * len(train_data))\n",
        "test_size = len(train_data) - train_size\n",
        "\n",
        "# 將訓練集和驗證集合併為訓練集\n",
        "train_data, val_data = torch.utils.data.random_split(train_data, [train_size, test_size])\n",
        "\n",
        "# 定義DataLoader，用於將資料集轉換為小批次數據\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# 定義模型\n",
        "class Net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 100)\n",
        "        self.fc2 = torch.nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# 定義損失函數和優化器\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 訓練模型\n",
        "for epoch in range(10):  # 訓練 10 個 epochs\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    \n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# 測試模型\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "id": "0oFtvrVAIMgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FgJFViOv3cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "# 轉換訓練資料\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# 載入MNIST手寫數字資料集，並將資料轉換為Tensor型態\n",
        "train_data = torchvision.datasets.MNIST(root='./data', train=True, \n",
        "                                        transform=transforms.ToTensor(), \n",
        "                                        download=True)\n",
        "test_data = torchvision.datasets.MNIST(root='./data', train=False, \n",
        "                                       transform=transforms.ToTensor(), \n",
        "                                       download=True)\n"
      ],
      "metadata": {
        "id": "9GyIfNOqK882"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_tJG4HXY5UQ",
        "outputId": "5f7c2632-fddc-471a-a437-b028630e61eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUu5fy5EY-N6",
        "outputId": "e05586e9-0ed0-4854-c7a9-b10d8ef2336b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ConcatDataset([train_data, test_data])"
      ],
      "metadata": {
        "id": "bk_2hV2SZAkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7b6hjnBbsa0",
        "outputId": "61ec8f3c-e928-4e97-ee58-bf8f29376064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_data, test_data = train_test_split(dataset, random_state=777, train_size=0.8)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "aES5c4I9b_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "9Ny0fKaqdk-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 100)\n",
        "        self.fc2 = torch.nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "bZnWN-YDd3RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoLayerNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr =1e-4, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "mdUJ4orfoz8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "\n",
        "    inputs,label = data #每次64個\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 100 == 0:    \n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch , i , running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "3xFzP4mculcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 測試模型\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        print(\"label size\", labels.size(0))\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        print(\"predicted\",predicted)\n",
        "        print(\"labels\",labels)\n",
        "        print(\"correct\", (predicted == labels).sum().item())\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n"
      ],
      "metadata": {
        "id": "18a52Newv4g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChIvEmRf1i9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}